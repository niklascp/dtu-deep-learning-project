{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - Convolutional neural network (CNN) using only geo-spatial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('./src')\n",
    "from data_utils import *\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train, user_val, user_test = train, val, test = train_test_data_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.concat([create_data_frame(*load_user_data(user), segmentation=True) for user in user_train]).reset_index(drop=True)\n",
    "data_val = pd.concat([create_data_frame(*load_user_data(user), segmentation=True) for user in user_val]).reset_index(drop=True)\n",
    "data_test = pd.concat([create_data_frame(*load_user_data(user), segmentation=True) for user in user_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation is not strictly needed for the CNN-only model, but in order to ensure the datesets accross all three models are the same we still perform it here, and purge all points that would otherwise been purged to build the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train[data_train['segment_ix'] >= 5]\n",
    "data_val = data_val[data_val['segment_ix'] >= 5]\n",
    "data_test = data_test[data_test['segment_ix'] >= 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example input data\n",
    "\n",
    "Here we only use the ``user`` and ``image_ix`` to lookup the precomputed image frame and time of day ``tod`` as input, output is the ``label``, where *0 = motion*, and *1 = stop*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>image_ix</th>\n",
       "      <th>tod</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>8</td>\n",
       "      <td>637</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>8</td>\n",
       "      <td>638</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>8</td>\n",
       "      <td>699</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>8</td>\n",
       "      <td>700</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>8</td>\n",
       "      <td>701</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user  image_ix  tod  label\n",
       "49      8       637    2      0\n",
       "50      8       638    2      0\n",
       "111     8       699    3      0\n",
       "112     8       700    3      0\n",
       "113     8       701    3      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[['user', 'image_ix', 'tod', 'label']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load precomputed image frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = {}\n",
    "for user_id in user_train + user_val + user_test:\n",
    "    with open(f'/mnt/array/valse_data/DeepLearning/Project/Pickle/images_list_{user_id}.pickle', 'rb') as f:\n",
    "        image_data[user_id] = np.stack(pickle.load(f), axis = 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example GIS image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.palettes import Set1_9 \n",
    "image_sample_ix = data_train[lambda x: (x['label'] == 0) & (x['delta_t'] <= 1)].sample(3, random_state = 42)[['user', 'image_ix']].values\n",
    "fig, ax = plt.subplots(figsize = (3*4, 4), ncols = 3)\n",
    "for j, (user_id, image_ix) in enumerate(image_sample_ix):\n",
    "    ax[j].set_title(f'User {user_id}, Frame {image_ix}')\n",
    "    for i, feature_ix in enumerate([4, 5, 3, 6, 7, 8, 10, 0, 2]):\n",
    "        feature = image_data[user_id][image_ix][:,:,feature_ix]\n",
    "        color = list(hex_to_rgb(Set1_9[i]))\n",
    "        color.append(200)\n",
    "        layer = np.stack([(feature >= 1) * c / 255. for c in color], axis = 2)\n",
    "        ax[j].imshow(layer, origin = 'lower')\n",
    "        ax[j].tick_params(\n",
    "            axis='both',       # changes apply to the x-axis\n",
    "            which='both',      # both major and minor ticks are affected\n",
    "            left=False,      # ticks along the bottom edge are off\n",
    "            right=False,         # ticks along the top edge are off\n",
    "            bottom=False,      # ticks along the bottom edge are off\n",
    "            top=False,         # ticks along the top edge are off\n",
    "            labelbottom=False,\n",
    "            labelleft=False) # labels along the bottom edge are off\n",
    "plt.savefig(\"context_frame.svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numer of training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Image Tensor Dataset\n",
    "\n",
    "We implement our own Tensor Dataset in order to be able to do fast lookup of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTensorDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, image_data):\n",
    "        self.labels = df['label'].values\n",
    "        self.tod = df['tod'].values\n",
    "        self.user_id = df['user'].values\n",
    "        self.image_ix = df['image_ix'].values        \n",
    "        self.image_data = image_data\n",
    "        self.tod_one_hot = np.eye(6)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        image = self.image_data[self.user_id[key]][self.image_ix[key]]\n",
    "        tod_one_hot = self.tod_one_hot[self.tod[key]]\n",
    "        return image, tod_one_hot, self.labels[key]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Input/output data from ``TensorDataset``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_img : torch.Size([3, 9, 9, 11])\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=torch.float64)\n",
      "X_tod : torch.Size([3, 6])\n",
      "tensor([0., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "y : torch.Size([3])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "demo_dataset = ImageTensorDataset(data_train.head(3), image_data)\n",
    "\n",
    "for X_img, X_tod, y in torch.utils.data.DataLoader(demo_dataset, batch_size=3, shuffle=True):\n",
    "    print('X_img :', X_img.shape)\n",
    "    print(X_img[0, :, :, 0])\n",
    "    print('X_tod :', X_tod.shape)\n",
    "    print(X_tod[0])\n",
    "    print('y :', y.shape)\n",
    "    print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_INPUT_H_W = 9, 9\n",
    "IMG_INPUT_C = 11\n",
    "TOD_INPUT_DIM = 6\n",
    "CNN_HIDDEN_DIM = 32\n",
    "CNN_HIDDEN_DIM_2 = 16\n",
    "CNN_KERNEL_SIZE = 3\n",
    "CNN_PADDING = 1\n",
    "FC_HIDDEN_DIM = 16\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "DROPOUT_PROP = .2\n",
    "\n",
    "NUM_EPOCH = 25\n",
    "BATCH_SIZE = 12000\n",
    "LEARNING_RATE = 0.1\n",
    "LEARNING_DECAY_FACTOR = 0.1\n",
    "LEARNING_DECAY_EPOCHS = [15,20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(ImageTensorDataset(data_train, image_data), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(ImageTensorDataset(data_val, image_data), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(ImageTensorDataset(data_test, image_data), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some utility functions for building and running the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, epoch):\n",
    "    number_decay_points_passed = np.sum(epoch >= np.array(LEARNING_DECAY_EPOCHS))\n",
    "    lr = LEARNING_RATE * (LEARNING_DECAY_FACTOR ** number_decay_points_passed)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def conv2d_output_shape(h_w, kernel_size=1, stride=1, padding=0, dilation=1):\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor( ((h_w[0] + (2 * padding) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * padding) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network for Convolutional neural network (CNN) using only geo-spatial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CnnNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(IMG_INPUT_C, CNN_HIDDEN_DIM, kernel_size=CNN_KERNEL_SIZE, padding=CNN_PADDING),\n",
    "            nn.BatchNorm2d(CNN_HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Dropout(DROPOUT_PROP)\n",
    "        )\n",
    "        \n",
    "        h, w = IMG_INPUT_H_W\n",
    "        #print(h, w)\n",
    "        h, w = conv2d_output_shape((h, w), CNN_KERNEL_SIZE, padding=CNN_PADDING)\n",
    "        #print(h, w)\n",
    "        h, w = conv2d_output_shape((h, w), kernel_size=3, stride=2, padding=0)        \n",
    "        #print(h, w)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(CNN_HIDDEN_DIM, CNN_HIDDEN_DIM_2, kernel_size=CNN_KERNEL_SIZE, padding=CNN_PADDING),\n",
    "            nn.BatchNorm2d(CNN_HIDDEN_DIM_2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(DROPOUT_PROP)\n",
    "        )\n",
    "        \n",
    "        h, w = conv2d_output_shape((h, w), CNN_KERNEL_SIZE, padding=CNN_PADDING)\n",
    "        #print(h, w)\n",
    "        h, w = conv2d_output_shape((h, w), kernel_size=2, stride=2, padding=0)\n",
    "        #print(h, w)\n",
    "        \n",
    "        self.fc1 = nn.Linear(CNN_HIDDEN_DIM_2 * h * w + TOD_INPUT_DIM, FC_HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(FC_HIDDEN_DIM, NUM_CLASSES)        \n",
    "\n",
    "    def forward(self, X_img, X_tod):\n",
    "        X_img = X_img.permute(0, 3, 1, 2)\n",
    "        out = self.layer1(X_img)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #print(out.shape)\n",
    "        #print(X_tod.shape)\n",
    "        out = torch.cat([out, X_tod], dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define class weights due to large class im-balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.3686257150152608]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = [(data_train['label'] == 0).sum()/(data_train['label'] == x).sum() for x in range(NUM_CLASSES)]\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model, optimizer and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1') # PyTorch v0.4.0\n",
    "model = CnnNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-10)\n",
    "criterion = nn.CrossEntropyLoss(weight = torch.Tensor(class_weights).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9090"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([np.product(x.cpu().detach().numpy().shape) for x in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test input/output of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([306, 2])\n"
     ]
    }
   ],
   "source": [
    "out = model(X_img.to(device, dtype=torch.float), X_tod.to(device, dtype=torch.float))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6037, 0.3963],\n",
       "        [0.5984, 0.4016]], grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:2].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = {\n",
    "    'train': {\n",
    "        'dataloader': train_dataloader,\n",
    "        'is_training': True,\n",
    "    },\n",
    "    'validation': {\n",
    "        'dataloader': val_dataloader,\n",
    "        'is_training': False,\n",
    "    },\n",
    "    'test': {\n",
    "        'dataloader': test_dataloader,\n",
    "        'is_training': False,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, NUM_EPOCH + 1):\n",
    "    \n",
    "    for phase_name, phase in phases.items():\n",
    "        epoach_loss = 0.0\n",
    "                \n",
    "        phase_is_training = phase['is_training']\n",
    "        phase_dataloader = phase['dataloader']\n",
    "        model.train(phase_is_training)\n",
    "        \n",
    "        # Ajust and save the learning rate in the phase dict\n",
    "        if phase_is_training:\n",
    "            lr = adjust_lr(optimizer, epoch)\n",
    "            phase.setdefault('lr', []).append(lr)\n",
    "            \n",
    "        # Iterate over each bath in the phase\n",
    "        for i, batch in enumerate(phase_dataloader, 1):\n",
    "            #print(i)\n",
    "            X_seq, X_tod, y = batch\n",
    "            \n",
    "\n",
    "            X_seq = X_seq.to(device, dtype=torch.float)\n",
    "            X_tod = X_tod.to(device, dtype=torch.float)\n",
    "            y = y.to(device, dtype=torch.int64)\n",
    "\n",
    "            with torch.set_grad_enabled(phase_is_training):\n",
    "                out = model(X_seq, X_tod)\n",
    "                loss = criterion(out, y)\n",
    "\n",
    "            if phase_is_training:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(model.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "        \n",
    "            epoach_loss += loss.item()\n",
    "        \n",
    "        # Save the loss for the epoch in the phase dict\n",
    "        phase.setdefault('loss', []).append(epoach_loss/i)\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(figsize = (16, 9))\n",
    "    #ax.set_yscale('log')\n",
    "    \n",
    "    for phase_name, phase in phases.items():\n",
    "        ax.plot(np.arange(1, epoch + 1), phase['loss'], label = phase_name)        \n",
    "    \n",
    "    ax.legend()\n",
    "    plt.show();            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation/Test prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase_name, phase in phases.items():\n",
    "    if phase['is_training']: \n",
    "        continue\n",
    "    \n",
    "    phase_dataloader = phase['dataloader']\n",
    "    phase['predicted'] = []\n",
    "    phase['true'] = []\n",
    "    \n",
    "    for i, batch in enumerate(phase_dataloader, 1):\n",
    "        X_img, X_tod, y = batch\n",
    "\n",
    "        phase['true'].extend(list(y.numpy()))\n",
    "        \n",
    "        X_img = X_img.to(device, dtype=torch.float)\n",
    "        X_tod = X_tod.to(device, dtype=torch.float)\n",
    "        y = y.to(device, dtype=torch.float)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model(X_img, X_tod)\n",
    "            probability, predicted = torch.max(out, 1)\n",
    "            \n",
    "        phase['predicted'].extend(list(predicted.cpu().numpy()))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase_name, phase in phases.items():\n",
    "    if phase['is_training']: \n",
    "        continue\n",
    "    \n",
    "    print(phase_name)\n",
    "    print(classification_report(phase['true'], phase['predicted'], target_names = ['Motion','Stop']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
