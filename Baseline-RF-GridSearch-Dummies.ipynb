{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from infostop import Infostop\n",
    "import pyproj\n",
    "import sklearn\n",
    "import pickle\n",
    "import torch\n",
    "sys.path.append('./src')\n",
    "from data_utils import *\n",
    "clear_output(wait=False)\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data clearnsing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_cutoff_speed = 45\n",
    "seq_cutoff_time = 60\n",
    "filter_seq = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to search for best performance of the random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "rfc=RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup machine learning baselines\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and test set which are rearranged in order to present a segments of 5 points with, distance between points, bearing rate and dummy variables for the geo-spatial context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(torch.utils.data.Dataset):\n",
    "\n",
    "        def __init__(self, df, filter_seq=filter_seq):\n",
    "            self.seq = np.stack([np.roll(df[['delta_d', 'bearing', 'f_highway_motorway','f_traffic_signals','f_bus_stops','f_landuse_meadow','f_landuse_residential','f_landuse_industrial','f_landuse_commercial','f_shop','f_railways','f_railways_station','f_subway']].values, i, axis = 0) for i in range(filter_seq, -1, -1)], axis = 1)\n",
    "            self.seq = self.seq[df['segment_ix'] >= filter_seq]\n",
    "\n",
    "            self.labels = df[df['segment_ix'] >= filter_seq]['label'].values        \n",
    "            self.user_id = df[df['segment_ix'] >= filter_seq]['user'].values\n",
    "            tod = df[df['segment_ix'] >= filter_seq]['tod'].values\n",
    "            self.tod_one_hot = np.eye(5)[tod]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "        def __getitem__(self, key):\n",
    "            return self.seq[key], self.tod_one_hot[key], self.labels[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train, user_val, user_test = train, val, test = train_test_data_split()\n",
    "user_train = user_train+user_val\n",
    "train=train+val\n",
    "\n",
    "data_train = pd.concat([create_data_frame(*load_user_data(user\n",
    "                                                          ,load_web_mercator = True\n",
    "                                                          , load_GPS = True\n",
    "                                                          , load_Dummies=True)\n",
    "                                          , segmentation=True\n",
    "                                          , seq_cutoff_time = seq_cutoff_time\n",
    "                                          , seq_cutoff_speed = seq_cutoff_speed) for user in user_train]).reset_index(drop=True)\n",
    "\n",
    "data_test = pd.concat([create_data_frame(*load_user_data(user\n",
    "                                                         ,load_web_mercator = True\n",
    "                                                         , load_GPS = True\n",
    "                                                         , load_Dummies=True)\n",
    "                                         , segmentation=True\n",
    "                                         , seq_cutoff_time = seq_cutoff_time\n",
    "                                         , seq_cutoff_speed = seq_cutoff_speed) for user in user_test]).reset_index(drop=True)\n",
    "\n",
    "data_train = data_train[data_train['segment_ix'] >= filter_seq]\n",
    "data_test = data_test[data_test['segment_ix'] >= filter_seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeSeries = TensorDataset(pd.concat([data_train]).reset_index(drop=True))\n",
    "XY_tr = np.array([np.concatenate(((TS[0]).reshape(-1),TS[1],TS[2].reshape(-1)), axis=0) for TS in TimeSeries]).reshape(-1,84)\n",
    "np.random.shuffle(XY_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = XY_tr[:,0:XY_tr.shape[1]-1]\n",
    "#X_tr = StandardScaler().fit_transform(X_tr)\n",
    "Y_tr = XY_tr[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed: 74.5min\n",
      "[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed: 207.0min\n",
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed: 248.6min finished\n"
     ]
    }
   ],
   "source": [
    "startGridSearch = time.time()\n",
    "#https://www.kaggle.com/sociopath00/random-forest-using-gridsearchcv\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5, n_jobs=-1, verbose=1)\n",
    "CV_rfc.fit(X_tr, Y_tr)\n",
    "endGridSearch = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check best performance of various parameters configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'criterion': 'entropy', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 100}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.898 (+/-0.001) for {'criterion': 'gini', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.899 (+/-0.001) for {'criterion': 'gini', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 200}\n",
      "0.898 (+/-0.001) for {'criterion': 'gini', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 500}\n",
      "0.898 (+/-0.001) for {'criterion': 'gini', 'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.899 (+/-0.001) for {'criterion': 'gini', 'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "0.898 (+/-0.001) for {'criterion': 'gini', 'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.898 (+/-0.001) for {'criterion': 'gini', 'max_depth': 4, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.898 (+/-0.001) for {'criterion': 'gini', 'max_depth': 4, 'max_features': 'log2', 'n_estimators': 200}\n",
      "0.898 (+/-0.001) for {'criterion': 'gini', 'max_depth': 4, 'max_features': 'log2', 'n_estimators': 500}\n",
      "0.899 (+/-0.001) for {'criterion': 'gini', 'max_depth': 5, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.899 (+/-0.001) for {'criterion': 'gini', 'max_depth': 5, 'max_features': 'auto', 'n_estimators': 200}\n",
      "0.899 (+/-0.001) for {'criterion': 'gini', 'max_depth': 5, 'max_features': 'auto', 'n_estimators': 500}\n",
      "0.899 (+/-0.001) for {'criterion': 'gini', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.899 (+/-0.001) for {'criterion': 'gini', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "0.899 (+/-0.001) for {'criterion': 'gini', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.899 (+/-0.001) for {'criterion': 'gini', 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.899 (+/-0.001) for {'criterion': 'gini', 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 200}\n",
      "0.899 (+/-0.001) for {'criterion': 'gini', 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 500}\n",
      "0.900 (+/-0.001) for {'criterion': 'gini', 'max_depth': 6, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.900 (+/-0.001) for {'criterion': 'gini', 'max_depth': 6, 'max_features': 'auto', 'n_estimators': 200}\n",
      "0.900 (+/-0.001) for {'criterion': 'gini', 'max_depth': 6, 'max_features': 'auto', 'n_estimators': 500}\n",
      "0.900 (+/-0.001) for {'criterion': 'gini', 'max_depth': 6, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.900 (+/-0.001) for {'criterion': 'gini', 'max_depth': 6, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "0.900 (+/-0.001) for {'criterion': 'gini', 'max_depth': 6, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.900 (+/-0.001) for {'criterion': 'gini', 'max_depth': 6, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.900 (+/-0.001) for {'criterion': 'gini', 'max_depth': 6, 'max_features': 'log2', 'n_estimators': 200}\n",
      "0.900 (+/-0.001) for {'criterion': 'gini', 'max_depth': 6, 'max_features': 'log2', 'n_estimators': 500}\n",
      "0.901 (+/-0.001) for {'criterion': 'gini', 'max_depth': 7, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.901 (+/-0.001) for {'criterion': 'gini', 'max_depth': 7, 'max_features': 'auto', 'n_estimators': 200}\n",
      "0.901 (+/-0.001) for {'criterion': 'gini', 'max_depth': 7, 'max_features': 'auto', 'n_estimators': 500}\n",
      "0.901 (+/-0.001) for {'criterion': 'gini', 'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.901 (+/-0.001) for {'criterion': 'gini', 'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "0.901 (+/-0.001) for {'criterion': 'gini', 'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.900 (+/-0.001) for {'criterion': 'gini', 'max_depth': 7, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.900 (+/-0.001) for {'criterion': 'gini', 'max_depth': 7, 'max_features': 'log2', 'n_estimators': 200}\n",
      "0.900 (+/-0.001) for {'criterion': 'gini', 'max_depth': 7, 'max_features': 'log2', 'n_estimators': 500}\n",
      "0.902 (+/-0.001) for {'criterion': 'gini', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.902 (+/-0.001) for {'criterion': 'gini', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 200}\n",
      "0.902 (+/-0.001) for {'criterion': 'gini', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 500}\n",
      "0.902 (+/-0.001) for {'criterion': 'gini', 'max_depth': 8, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.902 (+/-0.001) for {'criterion': 'gini', 'max_depth': 8, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "0.902 (+/-0.001) for {'criterion': 'gini', 'max_depth': 8, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.901 (+/-0.001) for {'criterion': 'gini', 'max_depth': 8, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.901 (+/-0.001) for {'criterion': 'gini', 'max_depth': 8, 'max_features': 'log2', 'n_estimators': 200}\n",
      "0.901 (+/-0.001) for {'criterion': 'gini', 'max_depth': 8, 'max_features': 'log2', 'n_estimators': 500}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 200}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 500}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.897 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.899 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'log2', 'n_estimators': 200}\n",
      "0.898 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'log2', 'n_estimators': 500}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'auto', 'n_estimators': 200}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'auto', 'n_estimators': 500}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 200}\n",
      "0.900 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 500}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 6, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 6, 'max_features': 'auto', 'n_estimators': 200}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 6, 'max_features': 'auto', 'n_estimators': 500}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 6, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 6, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 6, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 6, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 6, 'max_features': 'log2', 'n_estimators': 200}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 6, 'max_features': 'log2', 'n_estimators': 500}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'n_estimators': 200}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'n_estimators': 500}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.901 (+/-0.000) for {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'log2', 'n_estimators': 200}\n",
      "0.901 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'log2', 'n_estimators': 500}\n",
      "0.902 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.902 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 200}\n",
      "0.902 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 500}\n",
      "0.902 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 8, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.902 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 8, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "0.902 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 8, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.902 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 8, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.902 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 8, 'max_features': 'log2', 'n_estimators': 200}\n",
      "0.902 (+/-0.001) for {'criterion': 'entropy', 'max_depth': 8, 'max_features': 'log2', 'n_estimators': 500}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(CV_rfc.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = CV_rfc.cv_results_['mean_test_score']\n",
    "stds = CV_rfc.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, CV_rfc.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate performance on test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Motion       0.80      0.79      0.80     42456\n",
      "        Stop       0.93      0.93      0.93    122886\n",
      "\n",
      "    accuracy                           0.90    165342\n",
      "   macro avg       0.86      0.86      0.86    165342\n",
      "weighted avg       0.90      0.90      0.90    165342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TimeSeries = TensorDataset(pd.concat([data_test]).reset_index(drop=True))\n",
    "X_te = np.array([np.concatenate(((TS[0]).reshape(-1),TS[1]), axis=0) for TS in TimeSeries]).reshape(-1,83)\n",
    "#X_te = StandardScaler().fit_transform(X_te)\n",
    "Y_te = TimeSeries[:][2]\n",
    "\n",
    "startPrediction = time.time()\n",
    "y_true, y_pred = Y_te, CV_rfc.predict(X_te)\n",
    "endPrediction = time.time()\n",
    "\n",
    "cr=classification_report(Y_te, y_pred, target_names=['Motion','Stop'],output_dict=True)\n",
    "\n",
    "print(classification_report(Y_te, y_pred, target_names=['Motion','Stop']))\n",
    "\n",
    "#classification_test_performance.append(cr)\n",
    "#print(f'Gridsearch and Training lasted {endGridSearch-startGridSearch}')\n",
    "#print(f'Prediction lasted {endPrediction-startPrediction}')\n",
    "    \n",
    "\n",
    "#F1_macro_AVG = []\n",
    "#for cr in classification_test_performance:\n",
    "#    F1_macro_AVG.append(cr['macro avg']['f1-score'])\n",
    "#print(f'MEAN = {np.mean(F1_macro_AVG)}, STDEV = {np.std(F1_macro_AVG)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15171.945814847946\n",
      "2.8301570415496826\n"
     ]
    }
   ],
   "source": [
    "print(endGridSearch-startGridSearch)\n",
    "print(endPrediction-startPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
